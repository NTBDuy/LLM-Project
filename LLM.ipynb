{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NTBDuy/LLM-Project/blob/main/LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKf15vFwRb7U",
        "outputId": "ca641808-4b11-4c09-8e77-566b5012225d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/302.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/302.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.4/302.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q install transformers torch accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "bGY48TU8RnL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoConfig\n",
        "# load bộ tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "# cấu hình cho model\n",
        "config = AutoConfig.from_pretrained(\"google/flan-t5-base\", trust_remote_code = True)\n",
        "config.init_device = \"cuda\"\n",
        "config.temperature = 0.1\n",
        "config.max_length = 300\n",
        "# thông số mặc định\n",
        "config.eos_token_id = tokenizer.eos_token_id\n",
        "config.pad_token_id = tokenizer.pad_token_id\n",
        "config.do_sample = True\n",
        "# load model\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\", device_map = \"cpu\", config = config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTH1LTQuSZcS",
        "outputId": "d717cd97-08f8-4736-f73a-a25c29dbfc51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"sumary this text: ChatGPT is an artificial intelligence (AI) chatbox that uses natural language processing to create humanlike conversational dialogue\"\n",
        "\n",
        "# Tokenizer đoạn văn bản trên\n",
        "input_ids = tokenizer(input_text, return_tensors = \"pt\").input_ids.to(\"cpu\")\n",
        "\n",
        "# lấy output từ model\n",
        "outputs = model.generate(input_ids)\n",
        "\n",
        "# in ra output\n",
        "print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3hP64WRUDXI",
        "outputId": "9b6663aa-5565-40e6-9a1b-2f3c71fcf928"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1339: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pad> ChatGPT is a chatbox that uses natural language processing to create human-like conversational dialogue.</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"Answer the following question: can you tell me who is the president of US?\"\n",
        "\n",
        "# Tokenizer đoạn văn bản trên\n",
        "input_ids = tokenizer(input_text, return_tensors = \"pt\").input_ids.to(\"cpu\")\n",
        "\n",
        "# lấy output từ model\n",
        "outputs = model.generate(input_ids)\n",
        "\n",
        "# in ra output\n",
        "print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2NP19_qWAsl",
        "outputId": "7618dd40-c144-4d1c-bcf4-5574cfdcf207"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pad> Donald Trump</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"i am hungry, i want to eat something sweet, such as\"\n",
        "\n",
        "# Tokenizer đoạn văn bản trên\n",
        "input_ids = tokenizer(input_text, return_tensors = \"pt\").input_ids.to(\"cpu\")\n",
        "\n",
        "# lấy output từ model\n",
        "outputs = model.generate(input_ids)\n",
        "\n",
        "# in ra output\n",
        "print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6IHk6KxWVhd",
        "outputId": "eb0e44f2-f58a-4537-991b-eabccd48826d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pad> chocolate</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# call huggingface API\n",
        "import requests\n",
        "\n",
        "# Khai bao URL\n",
        "API_URL = \"https://api-inference.huggingface.co/models/google/flan-t5-base\"\n",
        "headers = {\"Authorization\": \"Bearer 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\"}\n",
        "\n",
        "#  Ham goi API va lay ket qua\n",
        "def query(payload):\n",
        "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
        "\treturn response.json()\n",
        "\n",
        "\n",
        "\n",
        "while True:\n",
        "  sentence = input(\"Enter a question:\")\n",
        "  if sentence == '': break\n",
        "  output = query({\n",
        "\t\"inputs\": sentence,\n",
        "  })\n",
        "  print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxUZY_-mWjj7",
        "outputId": "5a7660b3-168e-44c3-dac7-727b3880f7ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter a question:\n"
          ]
        }
      ]
    }
  ]
}